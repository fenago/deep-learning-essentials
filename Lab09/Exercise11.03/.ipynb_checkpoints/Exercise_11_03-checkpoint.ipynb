{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "303iHntmdiDj"
   },
   "source": [
    "# Exercise 11.3: Generating Images with DCGAN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8_-1h5ddiDp",
    "outputId": "5eb0a547-9432-4ca8-b442-49ddbb598442"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "    %tensorflow_version 2.x\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUutWi1Anz54"
   },
   "source": [
    "## **Import needed libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KubxTY1mdiDm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq4WgFjs6GtV"
   },
   "outputs": [],
   "source": [
    "def time_string(sec_elapsed):\n",
    "    hour = int(sec_elapsed / (60 * 60))\n",
    "    minute = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    second = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tb_XblE7diDr",
    "outputId": "9684a04e-1ec3-4263-ccbb-453149667e95"
   },
   "outputs": [],
   "source": [
    "gen_res = 3\n",
    "img_chan = 3\n",
    "gen_square = 32 * gen_res\n",
    "img_rows = 5\n",
    "img_cols = 5\n",
    "img_margin = 16\n",
    "seed_vector = 200\n",
    "data_path = 'apple-or-tomato/training_set/'\n",
    "epochs = 1000\n",
    "num_batch = 32\n",
    "num_buffer = 60000\n",
    "\n",
    "print(f\"Will generate a resolution of {gen_res}.\")\n",
    "print(f\"Will generate {gen_square}px square images.\")\n",
    "print(f\"Will generate {img_chan} image channels.\")\n",
    "print(f\"Will generate {img_rows} preview rows.\")\n",
    "print(f\"Will generate {img_cols} preview columns.\")\n",
    "print(f\"Our preview margin equals {img_margin}.\")\n",
    "print(f\"Our data path is: {data_path}.\")\n",
    "print(f\"Our number of epochs are: {epochs}.\")\n",
    "print(f\"Will generate a batch size of {num_batch}.\")\n",
    "print(f\"Will generate a buffer size of {num_buffer}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJ69ALfSdiDv",
    "outputId": "c6fafc24-fb72-4203-d6d2-410c4a9629ee"
   },
   "outputs": [],
   "source": [
    "training_binary_path = os.path.join(data_path,\\\n",
    "                                    f'training_data_{gen_square}_{gen_square}.npy')\n",
    "\n",
    "print(f\"Looking for file: {training_binary_path}\")\n",
    "\n",
    "if not os.path.isfile(training_binary_path):\n",
    "    start = time.time()\n",
    "    print(\"Loading images...\")\n",
    "\n",
    "    train_data = []\n",
    "    images_path = os.path.join(data_path,'tomato')\n",
    "    for filename in tqdm(os.listdir(images_path)):\n",
    "        path = os.path.join(images_path,filename)\n",
    "        images = Image.open(path).resize((gen_square,\n",
    "            gen_square),Image.ANTIALIAS)\n",
    "        train_data.append(np.asarray(images))\n",
    "    train_data = np.reshape(train_data,(-1,gen_square,\n",
    "            gen_square,img_chan))\n",
    "    train_data = train_data.astype(np.float32)\n",
    "    train_data = train_data / 127.5 - 1.\n",
    "\n",
    "\n",
    "    print(\"Saving training images...\")\n",
    "    np.save(training_binary_path,train_data)\n",
    "    elapsed = time.time()-start\n",
    "    print (f'Image preprocessing time: {time_string(elapsed)}')\n",
    "else:\n",
    "    print(\"Loading the training data...\")\n",
    "    train_data = np.load(training_binary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXl0JohJBx69"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data) \\\n",
    "                  .shuffle(num_buffer).batch(num_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Uc9clhao7ua"
   },
   "source": [
    "## **Build the Generator and Discrminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ulou-BZPybzT"
   },
   "outputs": [],
   "source": [
    "def create_generator(seed_size, channels):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
    "    model.add(Reshape((4,4,256)))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    if gen_res>1:\n",
    "        model.add(UpSampling2D(size=(gen_res,gen_res)))\n",
    "        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "    # Final CNN layer\n",
    "    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_discriminator(image_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n",
    "                     padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKnCeDut2cp0"
   },
   "outputs": [],
   "source": [
    "def save_images(cnt,noise):\n",
    "    img_array = np.full(( \n",
    "        img_margin + (img_rows * (gen_square+img_margin)), \n",
    "        img_margin + (img_cols * (gen_square+img_margin)), 3), \n",
    "        255, dtype=np.uint8)\n",
    "  \n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    img_count = 0\n",
    "    for row in range(img_rows):\n",
    "        for col in range(img_cols):\n",
    "            r = row * (gen_square+16) + img_margin\n",
    "            c = col * (gen_square+16) + img_margin\n",
    "            img_array[r:r+gen_square,c:c+gen_square] \\\n",
    "                = gen_imgs[img_count] * 255\n",
    "            img_count += 1\n",
    "\n",
    "          \n",
    "    output_path = os.path.join(data_path,'output')\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "  \n",
    "    filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
    "    im = Image.fromarray(img_array)\n",
    "    im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "gL5byGhNzOzd",
    "outputId": "4cc5eb88-934b-4a3f-f794-afb967699b3c"
   },
   "outputs": [],
   "source": [
    "generator = create_generator(seed_vector, img_chan)\n",
    "\n",
    "noise = tf.random.normal([1, seed_vector])\n",
    "gen_img = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(gen_img[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOnTxIXnyeEQ",
    "outputId": "90114667-2326-4b3b-bc1a-69223118242b"
   },
   "outputs": [],
   "source": [
    "img_shape = (gen_square,gen_square,img_chan)\n",
    "\n",
    "discriminator = create_discriminator(img_shape)\n",
    "decision = discriminator(gen_img)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBaP98zAySJV"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def discrim_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def gen_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79UDhOCa0R4h"
   },
   "outputs": [],
   "source": [
    "gen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzyh-LqU0j5d"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    seed = tf.random.normal([num_batch, seed_vector])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_imgs = generator(seed, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(gen_imgs, training=True)\n",
    "\n",
    "        g_loss = gen_loss(fake_output)\n",
    "        d_loss = discrim_loss(real_output, fake_output)\n",
    "\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(\\\n",
    "            g_loss, generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(\\\n",
    "            d_loss, discriminator.trainable_variables)\n",
    "\n",
    "        gen_optimizer.apply_gradients(zip(\n",
    "            gradients_of_generator, generator.trainable_variables))\n",
    "        disc_optimizer.apply_gradients(zip(\n",
    "            gradients_of_discriminator, \n",
    "            discriminator.trainable_variables))\n",
    "    return g_loss,d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjrRgDR10lSF"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    fixed_seed = np.random.normal(0, 1, (img_rows * img_cols, \n",
    "                                       seed_vector))\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        g_loss_list = []\n",
    "        d_loss_list = []\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            t = train_step(image_batch)\n",
    "            g_loss_list.append(t[0])\n",
    "            d_loss_list.append(t[1])\n",
    "\n",
    "        generator_loss = sum(g_loss_list) / len(g_loss_list)\n",
    "        discriminator_loss = sum(d_loss_list) / len(d_loss_list)\n",
    "\n",
    "        epoch_elapsed = time.time()-epoch_start\n",
    "        print (f'Epoch {epoch+1}, gen loss={generator_loss},disc loss={discriminator_loss},'\\\n",
    "           f' {time_string(epoch_elapsed)}')\n",
    "        save_images(epoch,fixed_seed)\n",
    "\n",
    "    elapsed = time.time()-start\n",
    "    print (f'Training time: {time_string(elapsed)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWmEHprD0t1V",
    "outputId": "070906d0-b2a1-40f5-dfd0-31b58af5daa9"
   },
   "outputs": [],
   "source": [
    "train(train_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "jprrfMKpB0Dz",
    "outputId": "75d064d9-47fe-4c28-8f2e-ada658e486d8"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-0.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "eGi6_OGm6nDr",
    "outputId": "a1fc01b9-4133-42f9-f681-65768a124ad4"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-1.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "Ciz8wwXg6nbS",
    "outputId": "654b4669-238b-4bff-b2ee-882ae33a859a"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-25.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "ypMzdlUd8W_H",
    "outputId": "53945352-c31c-483b-81b4-d835242b3267"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-50.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "iJA_-GZl8X3C",
    "outputId": "1ebce32b-05f5-472d-ecbe-804ab83a7aab"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-100.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "e6KRDZkb8diB",
    "outputId": "2412d847-3748-40c7-c76b-75edf2161be3"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-250.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "NP9Vqmkj8fh6",
    "outputId": "615f8b4f-9edc-4f0a-b357-89eec108ca47"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets'\\\n",
    "           '/apple-or-tomato/training_set/output/train-500.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "7qMSzAUq8hSd",
    "outputId": "e3abfa6b-ac58-4c7c-8608-8c1ef6ccb090"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets/apple-or-tomato'\\\n",
    "           '/training_set/output/train-750.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "3v2VOlVM8hq6",
    "outputId": "f2ea2bd3-2a7b-4736-feb5-0e3653fa0308"
   },
   "outputs": [],
   "source": [
    "a = imread('/content/drive/MyDrive/Datasets/apple-or-tomato'\\\n",
    "           '/training_set/output/train-999.png')\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15Hia_feD9sm"
   },
   "outputs": [],
   "source": [
    "# generator.save(os.path.join(DATA_PATH,\"tomato_generator.h5\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 11.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
