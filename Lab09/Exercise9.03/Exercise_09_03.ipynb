{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBRXoTy8_r5Y"
   },
   "source": [
    "# Exercise 9.03: Building an RNN with LSTM Layer for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzLzszzWD99d"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils as ku\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sIxuhC_S5ith",
    "outputId": "42e6a940-d11a-4f37-e2f9-b4920349a18e"
   },
   "outputs": [],
   "source": [
    "curr_dir = '../Datasets'\n",
    "all_headlines = []\n",
    "for filename in os.listdir(curr_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(\"Articles.csv\")\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "9AYJZcRN57xZ",
    "outputId": "da58f2d9-7375-49e2-a31b-5a7d7c0c5b6a"
   },
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt \\\n",
    "                  if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "wqitzE5r6Gp3",
    "outputId": "cbae31c4-7507-4590-e9e8-83235a371ed4"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_seq_of_tokens(corpus):\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    all_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, all_words\n",
    "\n",
    "inp_sequences, all_words = get_seq_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMbgh0JX6SfL"
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array\\\n",
    "                      (pad_sequences(input_sequences, \\\n",
    "                                     maxlen=max_sequence_len, \\\n",
    "                                     padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],\\\n",
    "                        input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=all_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences\\\n",
    "                                      (inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "hdkZNPn16fH-",
    "outputId": "b232d82b-dedd-45e8-c323-09eda0cbdba1"
   },
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, all_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(all_words, 10, input_length=input_len))\n",
    "    \n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(all_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \\\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, all_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pHltx5kz6fAi",
    "outputId": "9b06eadd-f8d3-49db-f190-2d3a7c2e3c20"
   },
   "outputs": [],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJYgZG9b6d1S"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, \\\n",
    "                  model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences\\\n",
    "                     ([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], \\\n",
    "                                   maxlen=max_sequence_len-1, \\\n",
    "                                   padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "               \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted.any():\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "1VSxnpR0Ln3Z",
    "outputId": "079574c9-e532-4cc7-a185-392549831bae"
   },
   "outputs": [],
   "source": [
    "print (generate_text(\"the hottest new\", 5, model, \\\n",
    "                     max_sequence_len))\n",
    "print (generate_text(\"the stock market\", 4, model, \\\n",
    "                     max_sequence_len))\n",
    "print (generate_text(\"russia wants to\", 3, model, \\\n",
    "                     max_sequence_len))\n",
    "print (generate_text(\"french citizen\", 4, model, \\\n",
    "                     max_sequence_len))\n",
    "print (generate_text(\"the one thing\", 15, model, \\\n",
    "                     max_sequence_len))\n",
    "print (generate_text(\"the coronavirus\", 5, model, \\\n",
    "                     max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise_9_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
