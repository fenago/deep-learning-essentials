{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBRXoTy8_r5Y"
   },
   "source": [
    "# Exercise 11.01: Generating Text \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzLzszzWD99d"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import tensorflow.keras.utils as ku \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "import string, os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIxuhC_S5ith",
    "outputId": "8fe69a66-2a9e-4ee9-cf73-0b80d15d2595"
   },
   "outputs": [],
   "source": [
    "our_dir = '../../Lab08/Datasets/'\n",
    "our_headlines = []\n",
    "for filename in os.listdir(our_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(our_dir + filename)\n",
    "        our_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "our_headlines = [h for h in our_headlines if h != \"Unknown\"]\n",
    "len(our_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AYJZcRN57xZ",
    "outputId": "e70562fb-0fe8-4215-8602-2be965913281"
   },
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in our_headlines]\n",
    "corpus[60:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqitzE5r6Gp3",
    "outputId": "37ad3c8a-d7c1-48f3-9e45-d7249a8d45d5"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_seq_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    all_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_seq = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_seq.append(n_gram_sequence)\n",
    "    return input_seq, all_words\n",
    "\n",
    "our_sequences, all_words = get_seq_of_tokens(corpus)\n",
    "our_sequences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMbgh0JX6SfL"
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_seq):\n",
    "    max_sequence_len = max([len(x) for x in input_seq])\n",
    "    input_seq = np.array(pad_sequences\\\n",
    "                         (input_seq, maxlen=max_sequence_len, \\\n",
    "                          padding='pre'))\n",
    "    \n",
    "    predictors, label = input_seq[:,:-1],input_seq[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=all_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(our_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdkZNPn16fH-",
    "outputId": "8abe64bf-3b55-43f6-8c68-2416ebbf4759"
   },
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, all_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(all_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(all_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, all_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHltx5kz6fAi",
    "outputId": "f2e194a6-6617-48f4-f8bd-ff031ed012fb"
   },
   "outputs": [],
   "source": [
    "model.fit(predictors, label, epochs=200, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJYgZG9b6d1S"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], \\\n",
    "                                   maxlen=max_sequence_len-1, \\\n",
    "                                   padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted.any():\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VSxnpR0Ln3Z",
    "outputId": "28a471a9-c628-4743-9b05-1a651c9e1da0"
   },
   "outputs": [],
   "source": [
    "print (generate_text(\"10 Ways\", 11, model, max_sequence_len))\n",
    "print (generate_text(\"europe looks to\", 8, model, max_sequence_len))\n",
    "print (generate_text(\"best way\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"homeless in\", 10, model, max_sequence_len))\n",
    "print (generate_text(\"Unexpected results\", 10, model, \\\n",
    "                     max_sequence_len))\n",
    "print (generate_text(\"critics warn\", 10, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 11.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
